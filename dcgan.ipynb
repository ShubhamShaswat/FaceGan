{"cells":[{"metadata":{"_uuid":"3998a914-aff7-4332-855d-1597a8f733c1","_cell_guid":"b7a08706-d539-4add-be91-48e01f0a47d5","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"72997fa7-c820-4753-afe0-858070c8508f","_cell_guid":"7ba82f5f-1651-44b5-ac85-079fa88650bf","trusted":true},"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nuser_credential = user_secrets.get_gcloud_credential()\nuser_secrets.set_tensorflow_credential(user_credential)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"a587abb1-84e4-407e-9241-bfd41ae07ad0","_cell_guid":"726e5b9d-5040-4a7b-98d4-2483543bb800","trusted":true},"cell_type":"code","source":"from google.cloud import storage\n\nSTORAGE_CLIENT = storage.Client(project='burnished-edge-278511')","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"b94ba03e-bc16-42ad-bb8f-6c7e0968de8a","_cell_guid":"d5308d9c-78c6-4473-8227-11d928523a91","trusted":true},"cell_type":"code","source":"from kaggle_datasets import KaggleDatasets\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"fdc5624d-6b1e-4142-af15-9b24c61ec48f","_cell_guid":"c8dbf277-e2ee-4ecd-a3a5-d75eb63f2258","trusted":true},"cell_type":"code","source":"GCS_DS_PATH","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"a9eb2e5d-b36b-44e7-a142-5f8dc6b681cd","_cell_guid":"342ff8f3-bef8-4d54-b315-337a46ac28d2","trusted":true},"cell_type":"code","source":"!gsutil ls $GCS_DS_PATH","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"01fe88b2-7c4a-4d83-b11f-ae35f9fc408a","_cell_guid":"c282a5ec-ca05-46a6-948b-4d15e0ff493c","trusted":true},"cell_type":"code","source":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nprint(\"Number of accelerators: \", tpu_strategy.num_replicas_in_sync)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"14146d8d-c383-4500-b7b1-32f112f58119","_cell_guid":"71464e3b-12dc-43c7-811b-7e5e712cb9a6","trusted":true},"cell_type":"code","source":"IMAGE_SIZE = [64,64]\n\n\n# Create a dictionary describing the features.\nimage_feature_description = {\n\n    'image': tf.io.FixedLenFeature([], tf.string),\n}\n\ndef _parse_image_function(example_proto):\n  # Parse the input tf.Example proto using the dictionary above.\n  parsed_example = tf.io.parse_single_example(example_proto, image_feature_description)\n  image = decode_image(parsed_example['image'])\n\n  return image\n\n\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n    image = tf.image.resize(image,[*IMAGE_SIZE])\n    return image","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"6c4e7ef9-57a0-4cb8-91d8-0d54914e7906","_cell_guid":"42e7ef43-6789-4cba-a2cf-4cd8512cbb0a","trusted":true},"cell_type":"code","source":"def view_image(ds):\n    image = next(iter(ds)) # extract 1 batch from the dataset\n    image = image.numpy()\n\n    fig = plt.figure(figsize=(20, 20))\n    for i in range(20):\n        ax = fig.add_subplot(4, 5, i+1, xticks=[], yticks=[])\n        ax.imshow(image[i])","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"60793608-0509-412d-85b5-bfc60d1a019a","_cell_guid":"fe9357b2-f12b-45d4-a236-acb8ac8ee7f1","trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 128\n\nignore_order = tf.data.Options()\nignore_order.experimental_deterministic = False\n    \n# On Kaggle you can also use KaggleDatasets().get_gcs_path() to obtain the GCS path of a Kaggle dataset\n#filenames = tf.io.gfile.glob(\"gs://celeba_bucket/*.tfrecord\")\ndata_dir = GCS_DS_PATH + '/*.tfrecord'\nfilenames = tf.io.gfile.glob(data_dir)\ndataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n\n\ndataset = dataset.map(_parse_image_function, num_parallel_calls=AUTO)\n#train_dataset = train_dataset.map(process_data, num_parallel_calls=AUTO)\ndataset = dataset.repeat().shuffle(1024).batch(BATCH_SIZE)\n#dataset = dataset.map(...) # TFRecord decoding here...","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"974b5a8f-00af-48cb-977b-e51b51ace521","_cell_guid":"d47d374a-f3f1-46fc-98bb-b2043aa1c2ed","trusted":true},"cell_type":"code","source":"view_image(dataset)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"ff658875-ddb4-4e81-858d-4e8992d4e9bc","_cell_guid":"706f66c6-c6ab-4b47-bf45-84d476a5e5c2","trusted":true},"cell_type":"code","source":"","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"33d8f42a-0ffb-4676-a292-594df2208c22","_cell_guid":"f5fa9518-4faa-49f7-be72-1dcf0dbc5236","trusted":true},"cell_type":"code","source":"class make_disc_block(tf.keras.Model):\n    def __init__(self,filters,kernel_size,strides):\n        super(make_disc_block,self).__init__()\n        \n        self.conv2d = Conv2D(filters,\n                             kernel_size,\n                             strides,\n                             padding='same')\n        \n        self.batch_norm = BatchNormalization()\n        \n        self.leaky_relu = LeakyReLU(0.2)\n        \n    def call(self,x):\n        \n        x = self.conv2d(x)\n        x = self.batch_norm(x)\n        x = self.leaky_relu(x)\n        return x","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"64fad056-df94-4d4d-aad6-bb7a649e76b0","_cell_guid":"dee6d518-f525-426d-8d25-4707d5bdc2d9","trusted":true},"cell_type":"code","source":"#create a discriminator model\n\nfrom tensorflow.keras.layers import Conv2DTranspose, BatchNormalization, Dense, LeakyReLU, Conv2D, Reshape, Flatten, Dropout\n\n#define discriminator\n\nclass Discriminator(tf.keras.Model):\n\n  def __init__(self,nodes=256*4*4):\n    super(Discriminator,self).__init__()\n    \n    self.disc_block_1 = make_disc_block(filters=64,kernel_size=(5,5),strides=(2,2))\n    self.disc_block_2 = make_disc_block(filters=128,kernel_size=(5,5),strides=(2,2))\n    self.disc_block_3 = make_disc_block(filters=256,kernel_size=(5,5),strides=(2,2))\n    self.disc_block_4 = make_disc_block(filters=512,kernel_size=(5,5),strides=(2,2))\n    self.disc_block_5 = make_disc_block(filters=1024,kernel_size=(5,5),strides=(2,2))\n   \n    self.dense1 = Dense(1)\n    self.flatten = Flatten()\n    self.dropout = Dropout(.4)\n   \n  def call(self,x):\n\n    x = self.disc_block_1(x)\n    x = self.disc_block_2(x)\n    x = self.disc_block_3(x)\n    x = self.disc_block_4(x)\n    x = self.disc_block_5(x)\n\n    x = self.flatten(x)\n    x = self.dropout(x)\n    x = self.dense1(x)\n\n    return x","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"82ff109c-4e69-410e-b787-5556db414650","_cell_guid":"699ed9ff-2fe2-4e5b-8591-4fbbf2b6ab21","trusted":true},"cell_type":"code","source":"class make_generator_block(tf.keras.Model):\n    \n  def __init__(self,filters,kernel_size=(5,5),strides=(2,2)):\n    super(make_generator_block,self).__init__()\n\n    self.conv2d_transpose = Conv2DTranspose(filters,\n                                            kernel_size,\n                                            strides,\n                                            padding='same',\n                                            )\n    self.batch_norm = BatchNormalization()\n    self.leaky_relu = LeakyReLU(0.2)\n        \n  def call(self,x):\n    x = self.conv2d_transpose(x)\n    x = self.batch_norm(x)\n    x = self.leaky_relu(x)\n    return x","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"c72cef30-ae05-45e0-8035-7f882f60d7b8","_cell_guid":"f40997c9-edff-4626-80da-60522b515c14","trusted":true},"cell_type":"code","source":"#define generator\nfrom tensorflow.keras.layers import Conv2DTranspose, BatchNormalization, Dense, LeakyReLU, Conv2D, Reshape\n\nclass Generator(tf.keras.Model):\n  def __init__(self,nodes=512*4*4):\n    super(Generator,self).__init__()\n\n    self.dense1 = Dense(nodes)\n    self.leaky_relu = LeakyReLU(0.2)\n    self.gen_block_1 = make_generator_block(filters=512)\n    self.gen_block_2 = make_generator_block(filters=256)\n    self.gen_block_3 = make_generator_block(filters=128)\n    self.gen_block_4 = make_generator_block(filters=64)\n    self.conv2d_transpose_final = Conv2DTranspose(3,(5,5),strides=(1,1),activation='tanh',padding='same') #experiment with tanh\n    self.reshape = Reshape((4,4,512))\n  \n  @tf.function\n  def call(self,z):\n\n    #input layer\n    z = self.dense1(z)\n    z = self.reshape(z)\n    z = self.leaky_relu(z)\n    \n    z = self.gen_block_1(z)\n    z = self.gen_block_2(z)\n    z = self.gen_block_3(z)\n    z = self.gen_block_4(z)\n\n    #final layer\n    out = self.conv2d_transpose_final(z)\n    return out","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"c5c6b142-a9a7-4603-822d-bd95560b7eb7","_cell_guid":"337a1ab1-a647-4837-a8fe-4759564c1027","trusted":true},"cell_type":"code","source":"#test generator\nnoise = tf.random.normal([1,100])\ngen = Generator()\ngen_img = gen(noise)\n\nprint(gen_img.shape)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"a91e5db6-2183-4f34-b92b-11fb60003d1a","_cell_guid":"2db8b580-80fe-4516-b0e4-a7a7dcb0727b","trusted":true},"cell_type":"code","source":"#define custom loss function","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"ed458339-1aa6-49f4-8131-dfc074483400","_cell_guid":"67c6d1cc-b0f0-47c5-8feb-df830c127cf8","trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n#BATCH_SIZE = 4096\ndef get_dataset(batch_size):\n  \n\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = False\n\n    # On Kaggle you can also use KaggleDatasets().get_gcs_path() to obtain the GCS path of a Kaggle dataset\n    filenames = tf.io.gfile.glob('gs://celeba_bucket/*.tfrecord')\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n\n    dataset = dataset.map(_parse_image_function, num_parallel_calls=AUTO)\n    dataset = dataset.repeat().shuffle(200000).batch(batch_size,drop_remainder=True).prefetch(AUTO)\n    \n    return dataset","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"62898d6e-679f-4222-9863-0d0e8204b490","_cell_guid":"5998a079-2a2d-432c-9087-5480b53ed06a","trusted":true},"cell_type":"code","source":"#define optimizwr\n#g_opt = tf.keras.optimizers.Adam(1e-4)\n#d_opt = tf.keras.optimizers.Adam(1e-4)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"4ee68851-0c2a-4dd0-aade-7651f17985d6","_cell_guid":"4f30e3a4-2769-45f4-8a3a-309429b3de49","trusted":true},"cell_type":"code","source":"checkpoint_dir = 'gs://celeba_bucket/training_checkpoints_2'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"df690612-fedd-4c76-835a-8613dca01ccf","_cell_guid":"1e7ba252-12c9-4b46-b0e9-baff9a9d9fe6","trusted":true},"cell_type":"code","source":"#let's write a Wasserstein loss\n\ndef critic_loss(real_output,fake_output):\n    critic(fake_output) - critic(real_output)\n    return\n\ndef gen_loss()\n    pass","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"0b5ec47f-9b67-4260-81ab-884725381a4a","_cell_guid":"334cf96d-4a3e-4b44-97dc-971abc3cd3a7","trusted":true},"cell_type":"code","source":"# instantiating the model in the strategy scope creates the model on the TPU\nEPOCHS = 100\nBATCH_SIZE = 4096\nSTEPS_PER_TPU_CALL = 202599 // BATCH_SIZE\nSTEPS_PER_EPOCH = 202599 // BATCH_SIZE\n\nwith tpu_strategy.scope():\n    \n    #define model\n    generator = Generator()\n    discriminator = Discriminator()\n    \n    #define optimizers\n    g_opt = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5) #0.0001\n    d_opt = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5) #0.0001\n    \n    \n    #define loss \n    gen_loss = tf.keras.metrics.Mean(name='gen_loss')\n    disc_loss = tf.keras.metrics.Mean(name='disc_loss')\n\n    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True,reduction=tf.keras.losses.Reduction.NONE)\n    def discriminator_loss(real_output, fake_output):\n        real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n        fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n        total_loss = real_loss + fake_loss\n        return tf.nn.compute_average_loss(total_loss, global_batch_size=BATCH_SIZE)\n    \n    def generator_loss(fake_output):\n        gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n        return tf.nn.compute_average_loss(gen_loss, global_batch_size=BATCH_SIZE)\n    \n    #define checkpoints\n    checkpoint = tf.train.Checkpoint(generator_optimizer=g_opt,\n                                 discriminator_optimizer=d_opt,\n                                 generator=generator,\n                                 discriminator=discriminator)\n    \n\nper_replica_batch_size =  BATCH_SIZE // tpu_strategy.num_replicas_in_sync\n\ntrain_dataset = tpu_strategy.experimental_distribute_datasets_from_function(lambda _:get_dataset(per_replica_batch_size))\n\n#define a train step\n\n@tf.function\ndef train_step(iterator):\n    \n    def step_fn(x):\n        \"\"\"The computation to run on each TPU device.\"\"\"\n        z = tf.random.normal([per_replica_batch_size,100])\n\n        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n            #get generator loss\n            Dx = discriminator(x,training=True)\n            Gz = generator(z,training=True)\n            DGz = discriminator(Gz,training=True)\n\n            g_loss = generator_loss(DGz)\n            #get discriminator loss\n            d_loss = discriminator_loss(Dx,DGz)\n\n        gradients_of_generator = gen_tape.gradient(g_loss, generator.trainable_variables)\n        gradients_of_discriminator = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n\n        g_opt.apply_gradients(zip(gradients_of_generator,generator.trainable_variables))\n        d_opt.apply_gradients(zip(gradients_of_discriminator,discriminator.trainable_variables))\n        \n        gen_loss.update_state(g_loss * tpu_strategy.num_replicas_in_sync) # * tpu_strategy.num_replicas_in_sync \n        disc_loss.update_state(d_loss * tpu_strategy.num_replicas_in_sync )\n        \n   \n    tpu_strategy.run(step_fn, args = (next(iterator),))","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"30d7bdf0-a00e-4561-8a8b-0896b2a51a2f","_cell_guid":"35c7954e-e776-4491-9edc-327c8266e1dd","trusted":true},"cell_type":"code","source":"ckpt_path = 'gs://celeba_bucket/training_checkpoints_2'\nmanager = tf.train.CheckpointManager(checkpoint, ckpt_path, max_to_keep=3)\ncheckpoint.restore(manager.latest_checkpoint)\nif manager.latest_checkpoint:\n    print(\"Restored from {}\".format(manager.latest_checkpoint))\nelse:\n    print(\"Initializing from scratch.\")","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"981e0ce4-b380-4ac2-83a1-1aa8f35813ab","_cell_guid":"eb202987-64e5-4456-9994-65c088e3d821","trusted":true},"cell_type":"code","source":"#define train\nimport IPython.display as display\nimport time\n\n\ngen_loss_hist = []\ndisc_loss_hist = []\n\nsteps_per_epoch = 202599 // BATCH_SIZE\ntrain_iterator = iter(train_dataset)\n\ndef train(dataset):\n \n  for epoch in range(EPOCHS):\n\n    #start time \n    start = time.time()\n    \n    for step in range(steps_per_epoch):\n\n      train_step(train_iterator)\n        \n      print('Epoch : {} Batch : {} G Loss: {} D Loss: {}'.format(epoch,g_opt.iterations.numpy(),gen_loss.result(),disc_loss.result()))\n      display.clear_output(wait=True)\n        \n    print('Time : {}'.format(time.time()-start))\n    \n    # Save the model every 15 epochs\n    '''\n    if (epoch + 1) % 100 == 0:\n      checkpoint.save(file_prefix = checkpoint_prefix)\n    '''    \n    \n    #append losses\n    gen_loss_hist.append(gen_loss.result())\n    disc_loss_hist.append(disc_loss.result())\n    \n    #reset loss states\n    gen_loss.reset_states()\n    disc_loss.reset_states()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"632c9390-d9a5-4e93-8760-6fa2dbc0bc4e","_cell_guid":"f70bd61c-fba5-4f38-a501-ed3588d2949a","trusted":true},"cell_type":"code","source":"train(train_dataset)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"643dc687-3139-48be-814e-b1deb60c0f97","_cell_guid":"aa0edf33-dce5-42b2-a534-dc67b9493c0a","trusted":true},"cell_type":"code","source":"plt.plot(np.arange(100),gen_loss_hist)\nplt.plot(np.arange(100),disc_loss_hist)\nplt.show()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"53240f77-2bd0-49ab-a976-51eb7da7393b","_cell_guid":"6261dc40-33c4-44ee-bf55-c42ee3af1f1c","trusted":true},"cell_type":"code","source":"tf.saved_model.save(\n    generator, 'gs://celeba_bucket/saved_model_3/my_model',\n    signatures=generator.call.get_concrete_function(\n        tf.TensorSpec(shape=[None, 100], dtype=tf.float32, name=\"inp\")))","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"d7ea85ac-bfa7-492c-b9e4-58c66305067b","_cell_guid":"37995d1e-d066-4623-8bd6-74b8fbe7fd85","trusted":true},"cell_type":"code","source":"sda","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"329009ff-4687-4236-ac25-525e9637d697","_cell_guid":"a9462791-5369-4025-8bd0-e26beb0c2cca","trusted":true},"cell_type":"code","source":"n=16\nseed = tf.random.normal([n,100])\ngenerated_samples = generator(seed)\nfig = plt.figure(figsize=(10,10))\n\nfor i in range(n):\n    \n    plt.subplot(4, 4, i+1)\n    plt.imshow(generated_samples[i])\n\nplt.show()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"0c48238c-b1ed-4082-91ba-443ef216cd46","_cell_guid":"4a570ef7-f273-484e-a949-a55b0a4f1528","trusted":true},"cell_type":"code","source":"n=4\n\nfname = ['gs://celeba_bucket/training_checkpoints_2/ckpt-10',\n         'gs://celeba_bucket/training_checkpoints_2/ckpt-11',\n         'gs://celeba_bucket/training_checkpoints_2/ckpt-12',\n         'gs://celeba_bucket/training_checkpoints_2/ckpt-12',\n         'gs://celeba_bucket/training_checkpoints_2/ckpt-14',\n         'gs://celeba_bucket/training_checkpoints_2/ckpt-15',\n         'gs://celeba_bucket/training_checkpoints_2/ckpt-16']\n\nfor c in range(7):\n    \n    checkpoint.restore(fname[c])\n    seed = tf.random.normal([n,100])\n    generated_samples = generator(seed)\n    fig = plt.figure(figsize=(10,10))\n\n    for i in range(n):\n    \n        plt.subplot(1, 4, i+1)\n        plt.imshow(generated_samples[i])\n\n    plt.show()","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"8f110312-99da-4c44-b554-76ef3e724269","_cell_guid":"25730e57-5f3a-49ce-bb19-b7f174260006","trusted":true},"cell_type":"code","source":"#custom training loop\n\nimport IPython.display as display\nimport time\n\n\ntrain_iterator = iter(train_dataset)\n\ngen_loss_hist = []\ndisc_loss_hist = []\nstep = 0\nepoch_steps = 0\nepoch = 0\n\nwhile True:\n    \n    # run training step\n    train_step(train_iterator)\n    epoch_steps += STEPS_PER_TPU_CALL\n    step += STEPS_PER_TPU_CALL\n\n    \n    print('Epoch : {} steps : {} G Loss: {} D Loss: {}'.format(epoch+1,epoch_steps,gen_loss.result(),disc_loss.result()))\n    display.clear_output(wait=True)\n    \n    if (step // STEPS_PER_EPOCH) > epoch:\n        \n        gen_loss_hist.append(gen_loss.result())\n        disc_loss_hist.append(disc_loss.result())\n    \n        # set up next epoch\n        epoch = step // STEPS_PER_EPOCH\n        epoch_steps = 0\n        #reset loss states\n        gen_loss.reset_states()\n        disc_loss.reset_states()\n        \n    \n    if epoch >= EPOCHS:\n        break","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"5a61eac9-3513-4ce5-8089-629853fb9726","_cell_guid":"3b2bf990-162f-4c08-af01-c33a507a0c88","trusted":true},"cell_type":"code","source":"train(train_dataset)","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"8cd1c8ba-5b43-4351-9df1-27b1b500b177","_cell_guid":"93f5f78a-f478-42b2-9385-ec65482774ff","trusted":true},"cell_type":"code","source":"!ls","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"6dd362b0-ed7d-40b1-b21f-16ae61b20901","_cell_guid":"79116901-9058-4238-8f20-a76b035fe1cc","trusted":true},"cell_type":"code","source":"!zip -r filename.zip saved_model","execution_count":0,"outputs":[]},{"metadata":{"_uuid":"f2c6cfe1-5af5-4d6d-add0-d4b4ba0eabdb","_cell_guid":"d2c7d5af-3ccd-405e-b091-44f193185cc7","trusted":true},"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'filename.zip')","execution_count":0,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}